
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Zero-shot Object Detection</title>
		<meta property="og:image" content="https://salman-h-khan.github.io/images/Fig1_ZSD_P.JPG"/>
		<meta property="og:title" content="Zero-shot Object Detection" />
  </head>

  <body>
    <br>
         <center><span style="font-size:42px">Zero-Shot Object Detection: Learning to <br> Simultaneously Recognize and Localize Novel
          Concepts </span> <br><br> 
          <table align=center width=900px>
	<tr>
	<td align=center width=100px>
	<center><span style="font-size:20px"><a href="https://sites.google.com/site/rshafin/home">Shafin Rahman</a></span></center>
	</td>
        
	<td align=center width=100px>
	<center><span style="font-size:20px"><a href="https://salman-h-khan.github.io/">Salman Khan</a></span></center>
	</td>
				
        <td align=center width=100px>
	<center><span style="font-size:20px"><a href="http://www.porikli.com/">Fatih Porikli</a></span></center></td>	
	</table>
	<br>
		 
	 <table align=center width=500px>
	 <tr>	 
         <td align=center width=350px>
	 <center><span style="font-size:20px">Australian National University, Data61-CSIRO</span>
	 </center></td>
	  </table>	 

	 <table align=center width=500px>
	 <tr><td align=center width=150px>
	 <center><span style="font-size:20px"><a href='https://github.com/salman-h-khan/ZSD_Release'> [GitHub]</a></span>
	 </center></td>
	 
         <td align=center width=350px>
	 <center><span style="font-size:20px"><a href='https://arxiv.org/abs/1803.06049'> [Paper + Supplementary Material]</a></span>
	 </center></td>
	  </table>
          </center>

<!-- <br><br> ------------------------------------------------------------------------------<hr> -->

  		  <br>
  		  <table align=center width=850px>
  			  <tr>
  	              <td width=500px>
  			<center><a href="https://salman-h-khan.github.io/images/Fig1_ZSD_P.JPG"><img src = "https://salman-h-khan.github.io/images/Fig1_ZSD_P.JPG" height="300px"></img></href></a><br>
			</center> </td>
                </tr>
  	              <td width=400px>
  		<center> 
		<span style="font-size:14px"><i>Zero-shot detection deals with a more complex label space (object labels and locations) with
                  	considerably less supervision (i.e., no examples of unseen classes). (a) Traditional recognition
			task only predicts seen class labels. (b) Traditional detection task predicts both
			seen class labels and bounding boxes. (c) Traditional zero-shot recognition task only
			predicts unseen class labels. (d) The proposed ZSD predicts both seen and unseen
			classes and their bounding boxes.</i>
		</center>
  	              </td>

  		  </table>

      	  <br>
	<!-- Abstract or short description -->
		  <hr>
		  Current Zero-Shot Learning (ZSL) approaches are restricted
		to recognition of a single dominant unseen object category in a test
		image. We hypothesize that this setting is ill-suited for real-world applications
		where unseen objects appear only as a part of a complex scene,
		warranting both the ‘recognition’ and ‘localization’ of an unseen category.
		To address this limitation, we introduce a new ‘Zero-Shot Detection’
		(ZSD) problem setting, which aims at simultaneously recognizing
		and locating object instances belonging to novel categories without any
		training examples. We also propose a new experimental protocol for ZSD
		based on the highly challenging ILSVRC dataset, adhering to practical
		issues, e.g., the rarity of unseen objects. To the best of our knowledge,
		this is the first end-to-end deep network for ZSD that jointly models
		the interplay between visual and semantic domain information. To overcome
		the noise in the automatically derived semantic descriptions, we
		utilize the concept of meta-classes to design an original loss function
		that achieves synergy between max-margin class separation and semantic
		space clustering. Furthermore, we present a baseline approach extended
		from recognition to detection setting. Our extensive experiments show
		significant performance boost over the baseline on the imperative yet
		difficult ZSD problem.	
		  <br><br>
		  <hr>

	  <!-- Code -->
 		<center><h1>Code</h1></center>
  		  <table align=center width=1000px>
  			<tr>
  	              <td align=center width=850px>
  			<center>
			<td><a href='https://github.com/salman-h-khan/ZSD_Release'><img class="round" style="height:350" src="https://salman-h-khan.github.io/images/Fig2_ZSD_P.JPG"/></a></td>
	  		</center>
	  		  	<!-- </br> -->
			</tr>
		  </table>

  		  <table align=center width=800px>
			  <tr><center> <br>
				<!-- <span style="font-size:28px">Code coming soon!</span></i>			  	 -->
				<span style="font-size:28px">&nbsp;<a href='https://github.com/salman-h-khan/ZSD_Release'>[GitHub]</a>

				<span style="font-size:28px"></a></span>
			  <br>
			  </center></tr>
		  </table>
      	  <br>
		  <hr>

  		  <!-- Paper -->
  		  <table align=center width=550>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <td><a href="https://arxiv.org/abs/1803.06049"><img class="layered-paper-big" style="height:175px" src="https://salman-h-khan.github.io/images/Fig4_ZSD_P.JPG"/></a></td>
				  <td><span style="font-size:14pt">S. Rahman, S. H. Khan, F. Porikli.<br>
						Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts.<br>
				  Arxiv, 2018.<br>
				   [<a href="https://arxiv.org/abs/1803.06049">Link to Paper</a>]</a>
				  <span style="font-size:4pt"><a href="https://arxiv.org/abs/1803.06049"><br></a>
				  </span>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <!-- <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="./resources/bibtex.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table> -->

		  <hr>

			<a name="results"></a>
			<center><h1>Results</h1></center>
			Our framework allows us to detect both seen and unseen object cateogries in natural images. Below we show some sample results on ILSVRC dataset where our method was able to correctly detect 'unseen' objects.

			<br><br><br>
			<table align=center width=1100px>
				<tr height="550px">
					<td valign="top" width=1000px>
					<center>
					<a href="https://salman-h-khan.github.io/images/Fig3_ZSD_P.JPG"><img img src = "https://salman-h-khan.github.io/images/Fig3_ZSD_P.JPG" height = "550px"></a><br>
					<span style="font-size:14px">All of the detected classes were not seen by the model during training.</span><br>
					</center>
					</td>

				</tr>
			</table>

	 <br><br>
			<hr>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  			<left>
	  		<center><h1>Citation</h1></center>
	  		  
			S. Rahman, S. H. Khan and F. Porikli, 
			“Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts,”
			arXiv preprint arXiv:1803.06049, 2018. 	<br><br>
			
						
			<font face="Courier New">			
			@article{rahman2018zeroshot, <br>
 			 title={Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts}, <br>
  			author={Rahman, Shafin and Khan, Salman and Porikli, Fatih}, <br>
  			journal={arXiv preprint arXiv:1803.06049}, <br>
  			year={2018} } </font>
						
			</left>
		</td>
			 </tr>
		</table>

		<br><br>


     <!-- Acknowledgements -->
		<hr>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  			<left>
	  		<center><h1>Ancknowledgement</h1></center>
	  		  
			This project page template was modified from <a href="https://richzhang.github.io/colorization/">this webpage</a>.
						
			</left>
		</td>
			 </tr>
		</table>

		<br><br>

		
</body>
</html>
